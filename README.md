# ENM 5310 – Collagen Fibril Orientation Prediction

**Authors:** Michael Chiou, Stephen Ching, Quan Vo  
**Course:** ENM 5310  
**Notebook:** `ENM5310_Project_Notebook.ipynb`

---

## Overview

This repository contains a supervised deep-learning pipeline for predicting **2D collagen fibril orientation fields** from grayscale microscopy images. The task is formulated as a dense, per-pixel orientation regression problem using a U-Net–style convolutional neural network.

Ground-truth fibril orientations are provided as sparse in-plane vectors and are converted into continuous orientation maps using a sin(2θ), cos(2θ) representation to properly handle π-periodicity.

The notebook implements the full workflow:
- Image preprocessing and inspection  
- Physics-consistent data augmentation  
- Ground-truth orientation map construction  
- Model training with architectural and loss-function ablations  
- Quantitative and qualitative evaluation  

---
## Repository Structure
## Repository Structure

```
.
├── ENM5310_Project_Notebook.ipynb                  # Main notebook
├── Training Data/                 # Original training images + labels (data branch)
│   ├── training*.tiff
│   └── fibril_orientation_2d_results.csv
│   └── fibril_orientation_2d_results_manual.csv
├── Augmented Data/                # Generated by this notebook
│   ├── *.tiff
│   └── augmented_labels.csv
├── Models Evaluation/             # Metrics and visualization outputs
│   ├── *_Loss.png
│   ├── *_Prediction.png
│   ├── All_Models_Comparison_Metrics.csv
│   └── All_Models_Comparison_Plots.png
└── README.md
```


**Note:**  
The training images and CSV label files are stored in the **Training Data** of this repository.

---

## Data Description

### Images
- Format: `.tiff`
- Type: Single-channel grayscale microscopy images
- Bit depth: Preserved during loading (normalized only for training)
- Naming convention: `training<ID>.tiff`

### Labels (CSV)
Each image has sparse fibril annotations containing:
- `filename`: image name  
- `pos`: pixel location `(y, x, 0)`  
- `u, v`: in-plane unit vector components  

Fibril orientation is directionless (θ ≡ θ + π), motivating the sin(2θ)/cos(2θ) encoding used throughout.

---

## Data Augmentation

Augmented training data are generated by:
1. Rotating images by `{0°, 90°, 180°, 270°}`
2. Rotating annotation coordinates and vectors consistently
3. Randomly cropping fixed-size patches
4. Preserving physically valid fibril orientations

An optional preprocessing branch applies **CLAHE (adaptive histogram equalization)** to evaluate robustness to contrast changes.

Outputs:
- Augmented `.tiff` images
- `augmented_labels.csv` with corrected positions and vectors

---

## Ground Truth Representation

Sparse fibril vectors are converted into dense orientation maps by:
1. Assigning vectors to local square regions
2. Averaging overlapping vectors
3. Computing the local orientation angle θ
4. Encoding as:
   - `sin(2θ)`
   - `cos(2θ)`

A binary mask is generated to restrict loss computation to labeled regions only.

---

## Model Architecture

The network is a **configurable residual U-Net** with:
- Shallow or deep encoder–decoder depth
- Residual blocks for training stability
- Two-channel output (`sin(2θ)`, `cos(2θ)`)
- L2-normalized output vectors

This is a regression problem on the unit circle, not classification.

---

## Training Experiments

The notebook performs an **8-model ablation study** across:
- Preprocessing: CLAHE vs no CLAHE
- Loss function:
  - Cosine similarity (geometry-aware)
  - Mean squared error on sin/cos channels
- Network depth: shallow vs deep

Each configuration is trained independently and evaluated using identical metrics.

---

## Evaluation Metrics

Reported metrics include:
- **Mean Angular Error (MAE)** in degrees
- **Success Rate** (% of predictions within 15°)
- **Spread Ratio** (prediction variance / ground truth variance)
- **Bias** (mean angular offset)

Additional outputs:
- Vector-field overlays comparing predictions to ground truth
- Training loss curves
- Summary comparison plots across all models

---

## Running the Notebook

### Environment
- Designed for **Google Colab**
- GPU strongly recommended

### Steps
1. Mount Google Drive when prompted
2. Ensure directory paths match your Drive structure
3. Run cells sequentially:
   - Raw image inspection
   - Data augmentation (optional but recommended)
   - Training and evaluation

**Warning:**  
Data augmentation is I/O intensive and may take several minutes.

---

## Acknowledgment

Notebook structure adapted from the ENM 5310 course GitHub template.  
All augmentation, modeling, and evaluation logic were developed by the authors.

